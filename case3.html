<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8">
        <title>Brinda's Portfolio</title>
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link href="https://fonts.googleapis.com/css2?family=Cormorant+Garamond:wght@300&family=Roboto:wght@300&display=swap" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@200&display=swap" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@200;300;400;600&display=swap" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@300;400;500;600&display=swap" rel="stylesheet">
        <link rel= "stylesheet" href= "style.css">
    </head>
    <body>
     <header>
        <a href="index.html" class="header-brand">B.R.i.N.Z</a>
        <nav>
            <ul>
                <li><a href="about.html">About</a></li>
                <li><a href="https://brindamehra.github.io/brinzy/document/cv-final.pdf" target="_blank">CV</a></li>
                <li><a href="projects.html"> Projects</a></li>
                <li><a href="scribbles.html"> Scribbles</a></li>

            </ul>
            <a href="brindaoutput.html" class="header-output">Projects</a>
        </nav>
     </header>
     <main>
    
        <section class= "case-vid">
            <div class="wrapper">

                <h2>  Get the Gist </h2>
                <img src="imgs/speech-writing.png" alt="Speech versus Writing">
                <article>
                    <h3 class="long-text"> Exploring The Potential Role of Gist in Text-Based Composition and Editing with Speech </h3>
                     <h4>Abstract</h4></a>
                        <p> Speech-based dictation systems are acknowledged for their faster input capabilities and their purported hands-and-eyes free experience in comparison to graphical user interfaces (GUI). However, despite this, dictation capabilities in modern voice-user interfaces, including speech-to-text transcription systems are heavily underused because of the time users must spend on manually editing their composition, the lack of support in the creation of their composition and the issues caused by the use of GUI design principles in the design of voice-user interfaces despite the major differences in their input modalities (Al-Aynati et al., 2003; Karat et al., 1999). By empirically testing theories from cognitive science about the properties of speech and memory, as well as user-centric design testing for user interfaces, I aim to explore how modern VUIs, specifically speech-to-text transcription systems can be redesigned to support composition and editing processes for users while simultaneously reducing their hands-and-eyes demand. A potential avenue of exploration focuses on the use of ‘gist’, inspired by theories from cognitive science that suggest that the human brain shows a preference for ‘fuzzy recall’ or the general semantic content of information over more precise, verbatim recall. </p>

                        <h4>Study Outcome</h4>

                        <p>This project proposes redefining the current approaches to editing in speech-based dictation systems by capitalizing on the natural fuzzy processing preference of the user, through redefining what systems consider ‘respeaking’. Instead of asking users to select text and then restate the utterance to edit the semantic content, we propose moving towards a gist-based text composition method that supplants the need to edit transcribed text through utterance duplication, inspired by newer interfaces that explore respeaking-inspired editing (Fan et al., 2021; Ghosh et al., 2020). In this scenario, respeaking transitions from a verbatim-trace retrieval activity to a fuzzy-trace retrieval activity, mimicking human cognitive processes and reducing the effort required to produce text. As a consequence, respeaking itself becomes a form of editing (or iterative drafting), since users would first produce an initial text that would allow the system to extract abstract semantic content and display it as a template for users to produce their final utterances. Apart from reducing cognitive effort, this type of a system would also allow users to plan their composition, which is not currently possible in modern systems given their tendency to transcribe verbatim, plausibly leading to the ability to produce longer compositions with greater ease while also reducing the amount of hands-and-eyes interaction with the interface since users would no longer have to spend a significant amount of time detecting and fixing errors. 

                            <strong> This project is still a work-in-progress, so stay tuned for updates!</strong> Ongoing sub-projects of this one include <a href="case4.html">Did I Say That?</a> and <a href="case5.html">Iterative Drafting Interface</a>.
                        </p>

                        <h4>Project In Collaboration With</h4>
                        <p>Dr. LIU Can (CityU), Scott JENSON (ex-Google) </p>
    
                </article>
            
             <div class="next-btn">
                    <a href="projects.html" class="back"> &laquo; Back to Projects</a>
                </div>
    

            </div>
        
    
        </section>
     </main>  

     <footer>
         <div class="wrapper-footer">

         <ul>
             <li>Website Design &copy; Brinda Mehra</li>
             <li>Powered by Github</li>
         </ul>
        
         <div class="footer-sm">

            <a href="#">
                <img src="imgs/twitter-logo.png" alt="twitter logo">
            </a>
        
             <a href="https://github.com/brindamehra">
                 <img src="imgs/github-logo.png" alt="github logo">
             </a>

             <a href="https://www.linkedin.com/in/brinda-m-5225b9120/">
                <img src="imgs/linkedin-logo.png" alt="linkedin logo">
            </a>
         </div>
        </div>
     </footer>
    </body>
   
</html>
